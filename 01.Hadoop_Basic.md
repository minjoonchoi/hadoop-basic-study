# **Hadoop Basic**

## **MapReduce : 맵리듀스**

### **데이터 처리 프로그래밍 모델**

    맵리듀스 작업은 '맵' 단계와 '리듀스' 단계로 구분된다.
    각 단계는 입력과 출력은 각각 Key-Value pair를 가진다.
    입/출력의 타입은 프로그래머가 설정한다.
    맵 함수와 리듀스 함수를 작성해야한다.
    하둡은 직렬화를 위해 자체적으로 기본 타입 셋을 제공한다. 

### **데이터 흐름**

    잡(Job)은 Client가 수행하는 작업의 단위로 입력 데이터, 맵리듀스 프로그램, 설정 정보로 구성된다.
    잡은 맵 태스크와 리듀스 태스크로 나누어 실행한다.

    하둡은 입력 데이터를 스플릿(Split)으로 분리하여 맵 태스크로 전달한다.
    스플릿의 크기가 작을수록 부하 분산에 효과적이지만, 너무 작으면 맵 태스크 생성 오버헤드가 커지는 단점이 있다.
    최적의 스플릿 크기는 HDFS 블록 크기이며, 단일 노드에 저장될 수 있는 최대 크기이기 때문이다.
    데이터 스플릿이 저장된 노드에서 맵 태스크가 실행될 때 네트워크 전송이 발생하지 않기 때문에 가장 빠르게 처리되며, 이를 데이터 지역성 최적화(data locality optimization)라고 한다.
    리듀스 태스크는 모든 맵 태스크가 만들어는 결과를 입력 데이터로 받기 때문에 데이터 지역성에 따른 장점은 없다.

    맵 태스크의 결과를 리듀스 태스크로 전달하는 과정을 셔플(shuffle)이라고 한다.
    셔플에서 발생하는 데이터 전송을 최소화 하기 위해서 맵의 결과를 처리하는 컴바이너 함수(combiner function)를 사용할 수 있다.

    맵 태스크의 결과는 노드의 로컬 디스크에 저장되고, 리듀스 태스크의 결과는 HDFS에 저장된다.
    리듀스 태스크 수는 입력크기와 상관 없이 지정할 수 있고, 그 수 만큼 최종 결과의 파티션 수가 결정되기 때문에 실행시간에 큰 영향을 준다.

## **HDFS(Hadoop Distributed File System) : 하둡 분산 파일 시스템**

### **Block : 블록**

    물리 디스크에서 block은 한 번에 읽고 쓸 수 있는 데이터 최대량이고, 기본적으로 512 B다.
    단일 디스크 파일시스템은 디스크 block보다 작은 데이터라도 저장될 때에는 한 block을 점유한다.

    HDFS에서 block은 기본적으로 128 MB로 굉장히 큰 사이즈인데 한 block 안에 많은 데이터를 담으면 데이터 탐색속도 측면에서 효율적이기 때문이다.
    HDFS는 데이터를 저장할 때 파일의 사이즈만큼 디스크를 사용한다.
    
    단일디스크 파일시스템에서는 파일 단위로 데이터가 관리되지만, HDFS에서 block 단위로 데이터를 관리하는데 이는 단일디스크의 용량보다 큰 사이즈의 파일을 허용하게 만들고, 데이터 청크와 메타데이터를 분리 관리할수 있게 하고, 데이터 복제에 용이하게 하여 고가용성을 확보할 수 있게 한다.

### **NameNode : 네임노드 / DataNode : 데이터노드**

    HDFS는 'Master-Worker' 패턴으로 동작하는 네임노드(Master)와 데이터노드(Worker)로 구성된다.
    HDFS 클라이언트가 사용자의 요청에 따라 네임노드와 데이터노드 사이에서 통신하고 파일시스템에 접근한다.

    네임노드는 파일시스템 트리와 파일 및 디렉토리에 대한 메타데이터를 로컬디스크에 영속적으로 저장한다.
    메타데이터에는 파일의 모든 block이 저장된 데이터노드를 가지며, 이 정보는 시스템이 시작할 때 각 데이터노드에서 수집되어 재구성된다.
    
    데이터노드는 클라이언트나 네임노드의 요청이 있을 때 블록을 저장 및 탐색하고 주기적으로 블록 정보를 네임노드에 전달한다.


### **HA(High Availability) : 고가용성**

    네임노드는 HDFS의 단일 고장점(SPOF : Single Point Of Failure)이다.

    네임노드는 메타데이터와 파일 블록의 매핑 정보를 보관하는 유일한 저장소 이기 때문에 장애가 발생하면 새로운 네임노드가 동작하기 전까지 파일시스템 전체가 동작하지 않는다.
    
    새로운 네임노드의 구동에서 데이터노드와 클라이언트 측면은 단순히 새로운 네임노드를 바라보도록 하면 완료이지만,
    새로운 네임노드는 네임스페이스 이미지를 로드하고, 에디트 로그를 업데이트하고, 모든 데이터노드에서 블록 정보를 보고받아 안전모드를 벗어날 때까지는 요청을 처리하지 못한다.

    하둡 2.X 는 위처럼 새로운 네임노드를 다시 구동하는 것 대신에 'Active-Stanby' 쌍의 네임노드를 사용한다.
    Active 네임노드에서 장애가 발생하면 Stanby 네임노드가 역할을 이어받는다.

    메타데이터를 로컬디스크와 원격 NFS 마운트 두 곳에 백업을 하고, 보조 네임노드를 운영하는 것이 필요하다.
    보조 네임노드의 역할은 네임노드의 에디트 로그가 너무 커지지 않도록 주기적으로 네임스페이스 이미지를 에디트로그와 병합해 새로운 네임스페이스 이미지를 만들고 이미지 복제본을 보관하는 것이다.

### **데이터 흐름**

    데이터 읽기

    클라이언트는 네임노드에 읽고자 하는 데이터의 블록위치를 요청하면 데이터의 복제본을 가진 데이터노드들의 정보를 반환한다.
    데이터노드의 정보를 확인해서 클라이언트 노드에서 물리적으로 가까운 순서로 정렬해 가장 가까운 데이터노드에 직접 접근해서 블록을 읽는다.
    클라이언트가 데이터를 읽는 중에 통신 장애가 발생하면 정렬된 데이터노드 목록에서 다음 데이터노드에 접근하여 다시 읽고, 장애가 발생한 데이터노드와 블록 정보를 네임노드에 보고한다.
    클라이언트가 데이터노드에 직접 접근해서 블록을 읽음으로써 클라이언트 수가 증가함에 따라 요청이 많더라도 네임노드로 인한 병목현상은 일어나지 않는다.

    데이터 쓰기

    클라이언트는 네임노드에 파일생성 요청을 보내면 네임노드는 동일한 파일 유/무 및 클라이언트의 권한 등을 검사하고 문제가 없다면 복제 수준에 맞게 블록을 저장할 데이터노드들의 정보를 반환한다.
    클라이언트는 데이터노드 목록에서 첫번째 노드에 직접 접근해 패킷을 보내 데이터를 쓰게되고 복제 수준에 따라 클러스터에서 비동기적으로 데이터노드간 복제가 일어난다.
    데이터노드의 패킷 처리가 완료되면 클라이언트에게 ack 응답을 보내게 되고, 모든 데이터노드에서 응답을 받게되면 파이프라인의 처리가 완료되고 클라이언트는 네임노드에 보고한다.

## **YARN(Yet Another Resource Negotiator)**

### **하둡 클러스터 자원 관리 시스템**

    하둡2에서 맵리듀스의 성능을 높이기 위해 도입되었으나 비 맵리듀스 프로그램도 지원한다.
    사용자 코드에서 직접 YARN API를 사용할 수 없고 맵리듀스, 스파크, 테즈와 같은 분산 컴퓨팅 프레임워크 같은 고수준 API를 통해 YARN 애플리케이션을 실행한다.
    피그, 하이브, 크런치 등은 YARN이 아니라 맵리듀스, 스파크 위에서 동작하는 처리 프레임워크이다.

    클러스터에서 유일한 리소스매니저와 모든 노드에서 구동되는 노드매니저가 있다.
    리소스 매니저는 클러스터 자원 사용량을 관리하고, 노드매니저는 애플리케이션이 구동될 컨테이너를 구동하고 모니터링한다.

### **YARN 애플리케이션 수행**

    클라이언트가 리소스매니저에게 애플리케이션 마스터(Application Master) 구동을 요청하면 리소스매니저는 애플리케이션 마스터를 구동할 수 있는 노드매니저를 찾아 요청을 전달한다.
    노드매니저는 컨테이너에서 애플리케이션 마스터 프로세스를 실행하고 애플리케이션 마스터는 계산량에 따라 리소스 매니저에 필요 자원을 요청하고 추가 할당된 노드매니저들과 함꼐 분산 처리를 수행한다.

    YARN 애플리케이션을 스파크를 통해 구동할 경우에는 필요한 클러스터의 고정 개수 executor를 처음에 전부 요청하고,
    맵리듀스를 통해 구동할 경우에는 처음에는 맵 태스크 컨테이너만 요청한 뒤 맵 태스크가 어느정도 끝나면 그 후에 동적으로 리듀스 태스크에 필요한 자원을 추가 요청한다.

### **애플리케이션 만들기**

    잡의 방향성 비순환 그래프(DAG : Directed Acyclic Graph) 실행은 스파크나 테즈가 적합하고,
    스트리밍 처리는 스파크, 쌈자(Samza) 또는 스톰(Storm)이 적합하다.

    아파치 슬라이더는 YARN 애플리케이션을 쉽게 실행할수 있게 해주는데 클러스터 사용자간 상호 독립적인 애플리케이션 구동을 가능하게 하여 여러 사용자가 동일한 애플리케이션의 다른 버전을 실행할 수 있게 한다.

### **YARN vs 맵리듀스 1**

    맵리듀스 1은 잡트래커와 태스크트래커가 있다.
    YARN은 리소스매니저, 애플리케이션 마스터, 노드매니저 등을 가진다.

    맵리듀스 1에서는 잡트래커가 태스크와 태스크트래커를 연결하는 스케줄링과 태스크 추적 및 실패한 태스크 재시작 그리고 카운터 관리를 포함하는 태스크 모니터링을 담당한다.
    YARN에서는 리소스매니저가 클러스터 자원 관리와 스케줄링하고 애플리케이션 마스터가 태스크 모니터링을 담당한다.

    맵리듀스 1에서는 태스크트래커가 태스크를 실행하고 진행 상황을 잡트래커에 보고한다.
    YARN에서는 노드매니저가 잡트래커 역할을 수행하고 애플리케이션 마스터에 보고한다.


    YARN은 맵리듀스 1에서 해결하지 못하는 여러 한계를 극복하기위해 설계되었다.
    
    확장성 : 맵리듀스 1에서는 잡트래커가 스케줄링과 태스크를 모두 담당하기 때문에 병목현상이 일어난다.
    YARN에서는 리소스매니저와 애플리케이션 마스터가 잡트래커의 역할을 나누어 수행한다.
    
    가용성 : 서비스 데몬의 장애복구를 위해서 상태정보를 복사해두는 방법으로 고가용성을 확보하는데 맵리듀스 1에서는 잡트래커가 가진 잡 스케줄링 및 태스크 모니터링 정보는 매우 빠르게 변경되기 때문에 복사저장하기 어렵다.
    YARN에서는 리소스매니저와 애플리케이션 마스터의 HA를 각각 제공하도록 기능을 설계 구현했다.

    효율성 : 맵리듀스 1에서는 태스크트래커가 고정 크기 '슬롯'이 존재하고 맵 슬롯과 리듀스 슬롯을 정적 할당하기 때문에 맵 슬롯에는 맵 태스크만 실행가능하고 리듀스 슬롯에서는 리듀스 태스크만 실행가능하기 때문에 슬롯 부족으로 병목이 발생할 수 있다.
    YARN에서는 노드매니저가 슬롯 대신에 제한된 리소스 풀을 관리하는데 태스크가 실행가능한 자원만 있으면 동작하기 때문에 병목이 발생하지 않는다.

    멀티테넌시 : 

## **하둡 I/O**

### **데이터 무결성**

    HDFS는 데이터를 쓸 때 내부적으로 체크섬을 계산하고 데이터와 함께 저장하고, 데이터를 읽을 때 체크섬을 검증한다.
    각 데이터노드는 백그라운드 스레드로 저장된 모든 블록을 주기적으로 검증한다.

### **Compression : 압축**

    파일 압축은 저장공간과 데이터 전송의 효율성을 위해 필요하다.
    압축과 해제가 빨라질수록 공간이 늘어나는 공간과 시간의 트레이드오프가 있다.

|압축포맷|알고리즘|확장자|분할가능|
|------|------|----|-----|
|DEFLATE|DEFLATE|.deflate|No|
|gzip|DEFLATE|.gz|No|
|bzip2|bzip2|.bz2|Yes|
|LZO|LZO|.lzo|No|
|LZ4|LZ4|.lz4|No|
|Snappy|Snappy|.snappy|No|

### **Serialization : 직렬화**

    직렬화는 네트워크 전송을 위해 객체를 바이트 스트림으로 전환하는 과정이다.
    하둡의 노드간 통신에서는 RPC 직렬화 포맷을 사용하고, 맵리듀스 프로그램의 키와 값 타입으로는 Writable이라는 자체 직렬화 포맷을 사용한다.
    하둡이 자바 객체 직렬화를 사용하지 않고 Writable을 사용하는 이유는 객체에 대한 제어능력 확보를 위해서이며, 프로세스 간 통신에서 자바 원격 메서드 호출(RMI : Remote Method Invocation)을 사용하지 않고 RPC를 사용하는 것도 통신 제어능력 확보를 위해서이다.
